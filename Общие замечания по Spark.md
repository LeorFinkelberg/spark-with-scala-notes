В отличие от Hadoop MapReduce, Spark не должен работать в тандеме с Apache Hadoop, хотя они часто используются вместе. Способность Spark реализовать _отложенные вычисления в оперативной памяти_ уникальна.

Сам по себе фреймворк Spark не предназначен для хранения данных, проводимые им вычисления на JVM (виртуальные машины Java) сохраняются только на время жизни приложения Spark.

Фреймворк может работать локально на отдельной машине с одной JVM (локальный режим). Но чаще он используется в сочетании с распределенной системой хранения данных (например, HDFS, Cassandra или S3) и диспетчером кластера для координации распределения приложений Spark по кластеру.

Ядром Spark является абстракция данных под названием Resilient Distributed Datasets (RDD, отказоустойчивые распределенные наборы данных). Набор RDD -- представление статически типизированных, распределенных коллекций с отложенными вычислениями.

### Модель параллельных вычислений фрейворка Spark. Наборы RDD

Вместо вычисления каждого преобразования сразу же после того, как его задаст программа-драйвер, Spark вычисляет RDD _отложенным образом_, рассчитывая результат преобразований набора только в момент, когда становятся необходимы итоговые данные RDD (зачастую для записи в хранилище или отправки агрегированных данных драйверу). Spark умеет хранить RDD _в оперативной памяти_ рабочих узлов во время всего времени жизни приложения Spark ради ускорения доступа при повторяемых вычислениях. Наборы RDD реализованы в Spark как _неизменяемые_, так что преобразование объекта RDD возвращает новый объект, а не уже существующий [[Литература#^20dbe7]]<c. 29>.

Набор RDD вычисляется полностью отложенным образом. Spark приступает к вычислению секций лишь при вызове _действия_.

NB! Далеко не все преобразования являются на 100% отложенными. Преобразованию `sortByKey` требуется вычислить RDD, чтобы определить диапазон данных, поэтому оно включает в себя и преобразование, и действие [[Литература#^20dbe7]]<c. 30>.

Отложенное вычисление позволяет Spark объединять операции, _не требующие взаимодействия с драйвером_ (их называют преобразованиями с взаимно однозначными зависимостями), чтобы избежать многократных проходов по данным.

Spark отличается отказоустойчивостью. Это значит, что он не подведет, не потеряет данные и не вернет неправильные результаты даже в случае отказа машины, на которой он работает, или сбоя сети. Уникальность обеспечения отказоустойчивости фреймворком Spark состоит в следующем: _каждая секция данных содержит информацию о зависимостях, достаточную для повторного ее вычисления_. Большинство парадигм распределенных вычислений, при которых пользователи могут работать с изменяемыми объектами, обеспечивают отказоустойчивость за счет журналирования обновлений или дублирования данных на других машинах. В отличие от них Spark не требует поддержания журнала обновлений каждого объекта RDD или журнала промежуточных шагов, поскольку _сам объект содержит всю информацию о зависимостях, необходимую для репликации всех его секций_ [[Литература#^20dbe7]]<c. 31>. Следовательно, при потере секции, в RDD есть достаточно информации относительно ее происхождения, чтобы вычислить ее заново, причем этот расчет можно распараллелить для ускорения восстановления. 
### Хранение данных в памяти и управление памятью

В значительной степени высокой производительности Spark обязан _хранению данных в оперативной памяти_. ==Вместо записи данных на диск== в промежутке между проходами Spark имеет возможность _хранить данные в оперативной памяти_. При этом данные каждой секции доступны в оперативной памяти всякий раз, когда они нужны.

### Функции, применяемые к наборам RDD. Преобразования и действия

Существует два типа определяемых для наборов RDD функций:
- действия,
- преобразования.

В каждой программе Spark должно содержаться действие, поскольку действия или передают информацию обратно драйверу, или записывают данные в устойчивое хранилище. Именно действия инициируют вычисление для программы Spark. Вызовы `.persist()`  тоже инициируют вычисления, но обычно не отмечают завершение задания. 

Действия, передающие информацию _обратно драйверу_: `.collect()`, `.count()`, `.collectAsMap()`, `.sample()`, `.reduce()` и `.take()` [[Литература#^20dbe7]]<c. 36>.

NB! Некоторые из этих действий ==плохо масштабируются==, так как могут приводить _к ошибкам памяти на драйвере_. В целом лучше использовать действия `.take()` , `.count()` и `.reduce()`, передающие обратно драйверу _данные фиксированного объема_, ==а не `.collect()` или `.sample()`== [[Литература#^20dbe7]]<c. 36>.

Среди действий, выполняющих запись в хранилище, -- `.saveAsTextFile()`, `.saveAsSequenceFile()` и `.saveAsObjectFile()`. Функции, ничего не возрващающие (`void` в Java или `Unit` в Scala), такие как `.foreach()`, тоже являются действиями: они инициализируют выполнение задания Spark.
### Широкие и узкие зависимости

Самая важная информация, которую нужно знать о преобразованиях для понимания механизма вычисления наборов RDD, -- то, что они делятся на две категории:
- преобразования _с узкими зависимостями_ (narrow dependencies): `.map()`, `.filter()`, `.mapPartitions()`, `.flatMap()`, `.coalesce()`.
- преобразования _с широкими зависимостями_ (wide dependencies): `.sort()`, `.sortByKey()`, `.reduceByKey()`, `.groupByKey()`, `.join()`, а также все преобразования, которые вызывают `.repartition()`.

NB! Другими словами, ==для широких преобразований требуется перетасовка==, а _для узких преобразований -- нет_ [[Литература#^20dbe7]]<c. 108>.

_Узкие зависимости не нуждаются в перемещении данных между секциями_! Следовательно, эти преобразования _не требуют обмена данными с узлом драйвера_, и получения одного набора инструкций от драйвера достаточно для совершения произвольного количества таких преобразований над любым подмножеством записей (любой секции). В терминологии Spark любая последовательность узких преобразований может быть вычислена на одном "этапе" плана выполнения запроса [[Литература#^20dbe7]]<c. 111>.

Связанная с широкой зависимостью перетасовка знаменует собой новый этап вычислений набора RDD. А поскольку задачи должны вычисляться по одной секции, а данные, необходимые для вычисления каждой секции широкой зависимости, могут быть распределены по нескольким машинам, преобразованиям с широкими зависимостями может понадобиться перемещать данные между секциями. А значит, расположенные далее по конвейеру вычисления нельзя произвести до завершения перетасовки.

Например, интуитивно понятно, что выполнить сортировку с помощью узких преобразований невозможно, поскольку она требует упорядочения всех записей, а не только в пределах каждой секции. И действительно, зависимости функции `sortByKey` -- широкие. Выполнить узкие преобразования, следующие за сортировкой, невозможно до завершения перетасовки, так как данные в любой из секций могут измениться.

Границы этапов существенно влияют на производительность. Таким образом, перетасовки ресурсоемки потому, что не только требуют перемещения данных и потенциально операций дискового ввода/вывода (для файлов перетасовок), но и ограничивают параллелизм [[Литература#^20dbe7]]<c. 111>.

По существу узкие преобразования -- это те, в которых у всех секций дочернего RDD имеются простые, конечные зависимости от секций родительского. Другими словами, секции при узких преобразованиях могут зависеть или от одного родителя (например, как в `.map()`, `.filter()`, `.mapPartitions()` или `.flatMap()`), или от уникального подмножества родительских секций, определенного уже во время проектирования (`.coalesce()`). Следовательно, узкие преобразования можно выполнять на произвольном подмножестве данных, не имея никакой информации о других секциях.

Напротив, преобразования с широкими зависимостями нельзя выполнять на произвольных строках, они требуют секционирования данных определенным образом, допустим, в соответствии со значением их ключей. Преобразования с широкими зависимостями включают `.sort()`, `.reduceByKey()`, `.groupByKey()`, `.join()`, а также все преобразования, которые вызывают метод `.repartition()` [[Литература#^20dbe7]]<c. 37>.

В некоторых случаях, когда Spark заранее знает, что данные секционированны определенным образом, операции с широкими преобразованиями не приводят к перетасовке. 
### Статическое и динамическое выделение ресурсов

Spark предоставляет два способа выделения ресурсов приложениям:
- статическое,
- динамическое.

При статическом каждому приложению выделяется конечный объем ресурсов кластера, который сохраняется за ним на протяжении всей жизни приложения. При динамическом выделении исполнители добавляются в приложение Spark и удаляются из него по мере необходимости, на основе набора эвристических правил для ожидаемой потребности в ресурсах [[Литература#^20dbe7]]<c. 308>.
### Приложение Spark

Каждый исполнитель соответствует виртуальной машине Java, так что не способен охватывать несколько узлов, хотя в одном узле может быть несколько исполнителей.

_Каждый исполнитель_ может иметь _несколько секций_, но ==секция не может распространяться на несколько исполнителей==.

Высокоуровневый слой планирования Spark использует зависимости RDD для построения ориентированного ациклического графа (DAG) этапов для каждого задания Spark.

Задание Spark (job) -- высший элемент иерархии выполнения фреймворка. _Каждое задание_ соответствует _одному действию_, а действия вызываются программой-драйвером.

На высоком уровне этап (stage) можно рассматривать как набор задач (task), каждое из которых выполняется одним исполнителем без взаимодействия с другими исполнителями или драйвером.

Иными словами, _граница нового этапа_ определяется _необходимостью сетевого взаимодействия между рабочими узлами_, например при перетасовке [[Литература#^20dbe7]]<c. 42>. Такие зависимости, формирующие границы этапов, называют перетасовочными (`ShuffleDependencies`). К перетасовкам приводят те из широких преобразований, которые требуют перераспределения данных по секциям, например как `.sort()` и `.groupByKey()`. Несколько преобразований с узкими зависимостями можно сгруппировать в один этап.

Этапы ограничиваются операциями перетасовки (`groupByKey()`, `sortByKey()`). Каждый этап состоит из нескольких задач, выполняемых _параллельно_: по одной для каждой секции.

При операциях над столбцами для равенства/неравенства применяются операторы `===` и `!==` с целью избежать конфликта с внутренними операторами Scala. Для столбцов строк доступны функции `startsWith/endsWith`, `substr`, `like` и `isNull`.

Для вычисления нескольких различных сводных показателей или более сложных показателей следует задействовать API `agg()` класса `GroupedData` вместо непосредственного вызова методов `.count()`, `.mean()` и пр. При использовании API `.agg()` необходимо указать или список выражений агрегирования, описывающих сводные показатели строки, или карту соответствий имен столбцов именам агрегирующих функций.
```scala
def minMeanSizePerZip(pandas: DataFrame): DataFrame = {
  pandas.groupBy(pandas("zip")).agg(
    min(pandas("pandaSize")),
    meann(pandas("pandaSize")) 
  )
}
```

Полный список полезных функций можно найти на https://spark.apache.org/docs/latest/sql-ref-functions-builtin.html

Можно писать запросы и без регистрации таблиц, указывая конкретный путь к файлу
```scala
def queryRawFile(): DataFrame = {
  sqlContext.sql("SELECT * FROM parquet.`path_to_partquet_file`")
}
```

Поскольку реализации JDBC у разных поставщиков СУБД несколько различаются, то понадобится добавить JAR-архив для вашего конкретного JDBC-источника данных. 

Включаем MySQL JDBC JAR
```bash
$ spark-submit \
  --jars ./resources/mysql-connector-java-5.1.38.jar $ASSEMBLY_JAR $CLASS
```
В случае _больших наборов данных рекомендуется сохранять данные по внешней системе хранения_ (такой база данных или HDFS). Как и в случае с RDD, ==не следует отправлять большие наборы DataFrame обратно на драйвер== [[Литература#^20dbe7]]<c. 80>.

Секционирование данных -- важная составляющая Spark SQL, поскольку оно необходимо для одной из ключевых оптимизаций: чтения только тех данных, которые нужны. Если все данные хранятся в одном объекте `DataFrame`, то можно легко задать информацию о секционировании во время записи данных с помощью API класса `DataFrameWriter`.  Метод `.partitionBy()` принимает в качестве параметра список столбцов, по которым необходимо секционировать результаты
```scala
def writeOutByZip(input: DataFrame): Unit = {
  input.write.partitionBy("zipcode").format("json").save("output/")
}
```

Одно из преимуществ наборов `Dataset` над обычными наборами `DataFrame` -- их _сильная типизация на этапе компиляции_. У `DataFrame` имеется информация о схеме на этапе выполнения, но нет сведений о ней на этапе компиляции. Такая сильная типизация особенно удобна при создании библиотек, поскольку позволяет задать более четкие требования к входным и возвращаемым типам [[Литература#^20dbe7]]<c. 85>.

`Dataset` -- это набор _типизированных объектов_, что означает, что _синтаксические ошибки_ (например, опечатка в имени метода) и _аналитические ошибки_ (например, неверный тип входной переменной) будут замечены еще _на этапе компиляции_. 

`DataFrame = Dataset[Row]` -- это набор _нетипизированных Row-объектов_ и потому на этапе компиляции могут быть замечены только синтаксические ошибки.

Что касается UDF, то ==даже написанные на JVM-языках UDF работают обычно медленее, чем работали бы эквивалентные SQL-выражения== [[Литература#^20dbe7]]<c. 88>.

Для UDAF вместо написания обычной функции языка Scala приходится расширять класс `UserDefinedAggregateFunction` и реализовать несколько различных функций. Будучи довольно сложными в написании, пользовательские функции агрегирования демонстрируют хорошую производительность по сравнению с такими вариантами, как функции `mapGroups` для класса `Dataset` или даже просто `aggregateByKey` для наборов RDD. Можно или вызывать UDAF непосредственно для столбцов, или добавить ее в реестр функций [[Литература#^20dbe7]]<c. 88>.

Пример UDAF для вычисления среднего значения
```scala
def setupUDAFs(sqlCtx: SQLContext) = {
  class Avg extends UserDefinedAggregateFunction {
    // Входной тип
    def inputSchema: org.apache.spark.sql.types.StructType = 
      StructType(StructField("value", DoubleType) :: Nil) 

    // Схема буфера
    def bufferSchema: StructType = StructType(
      StructField("count", LongType) ::
      StructField("sum", DoubleType)  :: Nil 
    )

    // Возвращаемый тип
    def dataType: DataType = DoubleType

    def deterministic: Boolean = true

    def initialize(buffer: MutableAggregationBuffer): Unit = {
      buffer(0) = 0L // инициалиация первого элемента буфера - счетчика
      buffer(1) = 0.0  // инициализация второго элемента буферф - суммы
    }

    def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
      buffer(0) = buffer.getAs[Long](0) + 1
      buffer(1) = buffer.getAs[Double](1) + input.getAs[Double](0) 
    }

    def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
      buffer1(0) = buffer1.getAs[Long](0) + buffer2.getAs[Long](0)
      buffer1(1) = buffer1.getAs[Double](1) + buffer2.getAs[Double](1) 
    }

    def evaluate(buffer: Row): Any = {
      buffer.getDouble(1) / buffer.getLong(0) 
    }

    // Необязательная регистрация
    val avg = new Avg
    sqlCtx.udf.register("ourAvg", avg)
  }
}
```

Наборы `DataFrame` уместны при наличии в основном реляционных преобразований, которые при необходимости можно расширить пользовательскими функциями UDF. По сравнению с наборами RDD, у DataFrame есть преимущество применения эффективного формата хранения Spark SQL, оптимизатора Catalyst и возможности выполнять некоторые операции непосредственно с сериализованными данными. Один из недостатков работы с наборами `DataFrame` -- отсутствие сильной типизации на этапе компиляции, что способно привести к ошибкам неправильного доступа к столбцам и прочим элементарным ошибкам [[Литература#^20dbe7]]<c. 93>.

Наборы `Dataset` можно использовать при необходимости комбинировать функциональные и реляционные преобразования с сохранением выгод оптимизаций наборов `DataFrame`. Наборы `Dataset` параметризуются типом содержащихся в них данных. Благодаря этому возможна сильная проверка типов во время компиляции, однако необходимо знать тип данных во время компиляции.

Чистые наборы RDD отлично работают с данными, которые не подходят для оптимизатора Catalyst. 

Соединения вообще весьма затратные операции, ведь соответствующие ключи всех RDD должны располагаться в одной и той же секции для возможности соединять их локально. При отсутствии в двух наборах RDD информации о секционировании их придется перетасовывать, чтобы метод секционирования (объект `Partitioner`) у них был один и тот же и данные с одинаковыми ключами находились в одних секциях. Как и при большинстве операций с данными типа "ключ-значение", стоимость соединения растет пропорционально количеству ключей и расстоянию, на которое следует переместить записи, чтобы они попали в нужную секцию.

Для соединения данных Spark нужно, чтобы соединяемые данные (от есть относящиеся к одному ключу) находились в одной секции.

Если нужно совершить перед соединением операцию, требующую перетасовки (такую как `aggregateByKey` или `reduceByKey`), то можно избежать перетасовки. Для этого следует добавить объект типа `HashPartitiioner` с тем же количеством секций в виде явного аргумента первой операции и сохранить набор RDD до выполнения соединения.

_Хеш-соединение с трансляцией_ (broadcast hash join) "вытакливает" один из наборов RDD (_меньший_) в каждый из рабочих узлов. Там при отображении совершается агрегирование с каждой секцией большого набора.

NB! В Spark Core отсутствует реализация хеш-соединения с трансляцией!

Модуль Spark SQL поддерживает те же базовые типы соединений, что и Spark Core,  но оптимизатор существенно облегчает задачу, хотя и лишает части возможностей, позволяющих контролировать происходящее. Например, Spark SQL иногда спускает операции на уровень хранилища или меняет порядок их выполнения ради повышения производительности соединений. С другой стороны, нельзя контролировать секционирование наборов `DataFrame` или `Dataset`, поэтому не получится вручную избежать перетасовок, как в случае соединений Spark Core [[Литература#^20dbe7]]<c. 102>.

_Физически данные хранятся в разделах/партициях_, а не во фреймах данных! [[Литература#^008892]]<c. 84>. Создаются разделы, и данные автоматически распределяются по всем разделам на основе существующей инфрастуктуры.

Левое полусоединение (LEFT SEMI JOIN), а также левое антисоединение (LEFT ANTI JOIN) -- единственные типы соединений, в результате которых выводятся значения только из левой таблицы.

Левое полусоединение аналогично фильтрации левой таблицы, при которой оставляются только строки с ключами, присутствующими в правой. Левое антисоединение тоже возвращает только данные из левой таблицы, но лишь записи, отсутствующие в правой.

Узнать тип выполняемого соединения в Spark SQL позволяет вызов `queryExecution.executedPlan`. Как и в случае Spark Core, если одна из таблиц намного меньше другой, то лучше воспользоваться _хеш-соединением с трансляцией_. Сообщить Spark SQL о необходимости выполнить трансляцию заданного набора при соединении можно, вызвав для этого набора `DataFrame` метод `broadcast`  перед соединением, например `df1.join(broadcast(df2), "key")`. Spark также автоматически применяет параметр `spark.sql.conf.autoBroadcastJoinThreshold`, чтобы определить, следует ли транслировать таблицу [[Литература#^20dbe7]]<c. 105>.

Соединить наборы `Dataset` можно с помощью метода `joinWith`, ведущего себя аналогично обычному реляционному соединению, за исключением того, что результат представляет собой кортеж различных типов записей.

При смене секции/партиции, к которой относится запись, перемещение данных между рабочими узлами необходимо, ведь _записи должны пройти через драйвер_, а не передаваться напрямую между исполнителями [[Литература#^20dbe7]]<c. 110>.

==Лучше избегать изменяемых структур данных в коде Spark== (да и вообще в коде на языке Scala), поскольку они могут приводить к ошибкам сериализации и неправильным результатам.

Один из важных способов оптимизации заданий Spark как в смысле памяти, так и времени выполнения -- стараться задействовать _простые типы данных_, а не пользовательские классы. Хотя от этого может пострадать удобочитаемость кода, применение массивов вместо case-классов или кортежей понижает затраты на сборку мусора. Массивы языка Scala, по сути являющиеся массивами языка Java, -- наиболее эффективный по памяти тип коллекций Scala. _Кортежи Scala_ представляют собой ==объекты==, так что в некоторых случаях вместо кортежа для ресурсоемких операций предпочтительнее задействовать двух- или трехэлементный _массив_ [[Литература#^20dbe7]]<c. 118>.

Функции внутри `.mapPartitions()` не должны требовать загрузки всей секции в оперативную память (например, из-за неявного преобразования ее в список) [[Литература#^20dbe7]]<c. 120>.

Использование преобразований "итератор - итератор" в программах Spark выгодно прежде всего тем, что _позволяет избирательно сбрасывать данные на диск_. По сути, это преобразование означает установку процесса _вычисления элементов по одному_. Следовательно, Spark может применять эту процедуру к группам записей, _а не читать всю секцию в оперативную память_ или создавать в оперативной памяти коллекцию из всех выходных записей с последующим ее возвращением. Соответственно, благодаря преобразованиям "итератор - итератор" _Spark может работать с секциями/партициями, не помещающимися в оперативной памяти отельного рабочего узла_ [[Литература#^20dbe7]]<c. 123>.

Преобразования "итератор - итератор" позволяют _вместо сброса на диск всей не помещающейся в оперативной памяти секции, сбрасывать только не помещающиейся записи_, а значит экономить операции дискового ввода/вывода и затраты на повторное вычисление [[Литература#^20dbe7]]<c. 123>.

В Spark имеется два вида разделяемых переменных (каждая из которых может записываться только в одном контексте -- драйвер или рабочий узел):
- транслирующие (broadcast) переменные: записываются на драйвере, а читаются на рабочих узлах; значение транслирующей переменной должно быть локальным и сериализуемым,
- накопительные (accumulator) переменные: записываются на рабочих узлах, а читаются на драйвере; могут вести себя непредсказуемо; не предназначены для сбора больших объемов информации. 

_Транслирующие переменные_ -- это способ распространения предназанченный только для чтения копии локального значения с драйвера на все машины вместо передачи новой копии с каждой задачей.

Два распространенных примера использования транслирующих переменных [[Литература#^20dbe7]]<c. 129>:
- трансляция небольшой таблицы для соединения,
- трансляция модели машинного обучения для формирования прогнозов на основе наших данных.

Транслирующая переменная создается с помощью вызова метода `broadcast` объекта `SparkContext`. Это приводит к распространению значения по рабочим узлам и возвращает адаптер, который можно использовать для доступа к этим значениям путем обращения к `value`. Если транслирующая переменная была создана с изменяемыми входными данными, то ==последние не должны меняться после ее создания==, ведь уже существующие экземпляры Spark этих обновлений не заметят, в то время как новые рабочие узлы увидят новое значение.

NB! Значение транслирующей переменной должно быть локальным и сериализуемым: никакие наборы RDD или другие распределенные структуры данных не допускаются [[Литература#^20dbe7]]<c. 130>. Когда транслирующая переменная больше не нужна, ее можно явным образом удалить, вызвав для нее метод `.unpersist()` .

_Накопители_ -- позволяют накапливать на рабочих узлах побочную информацию от преобразования или действия с последующей передачей результата обратно драйверу. В соответствии с моделью выполнения Spark приращивает накопители только при запуске (скажем, действием) вычислений. Если последние выполняются несколько раз, то фреймворк каждый раз обновляет накопитель. 

NB! ==Накопители могут вести себя непредсказуемым образом==. В их нынешнем состоянии их лучше использовать там, где возможный многократный подсчет является желательным поведением.

==Накопители не предназначены для сбора больших объемов информации==, так что при добавлении значительного количества элементов в коллекцию или присоединении к строке лучше задействовать отдельное действие вместо накопителя.

Все виды сохранения (один из которых -- кэширование) и создания контрольных точек означают определенные вычислительные затраты и вряд ли повысят производительность однократных операций. Более того, в случае больших наборов данных затраты на сохранение или создание контрольных точек могут оказаться настолько высоки, что лучшим выходом было бы повторное вычисление. Однако для некоторых особых видов программ повторное использование набора RDD может привести к колоссальному выигрышу в производительности как в смысле повышения скорости работы, так и в области снижения частоты отказов.

Вообще говоря, важнейшие сценарии повторного использования -- применение набора RDD несколько раз, выполнение нескольких действий над одним набором, а также длинные (или требующие больших вычислительных затрат) цепочки преобразований.

Итеративные вычисления. Например, сохранение набора данных, с которым выполняется цикл соединений, может привести к колосальному повышению производительности, поскольку гарантирует наличие секций данного набора RDD в оперативной памяти при всех соединениях.

Не рекомендуется разбивать граф происхождения между узкими преобразованиями, так как это приведет к разрыву конвейера [[Литература#^20dbe7]]<c. 139>.

Наилучший способ определить, следует ли повторно использовать RDD, -- запустить выполнение задания. Если процесс идет очень медленно, стоит взглянуть, не поможет ли сохранение  набора RDD, прежде чем пытаться переписать программу, ведь сохранение и создание контрольной точки способно снизить затраты на повторное вычисление данных в случае отказа или вообще их устранить. При сбое задания из-за ошибок сборки мусора или недостатка памяти создание контрольной точки или сохранение вне кучи может позволить завершить задание, особенно когда кластер зашумлен. С другой стороны, если вы уже используете сохранение, причем в оперативной памяти, то имеет смысл задуматься об удалении вызова операции сохранения или переходе на создание контрольной точки или сохранение/создание контрольной точки вне кучи [[Литература#^20dbe7]]<c. 139>.

Кэширование наболее удобно для исключения повторного вычисления в одном задании Spark или для разбиения наборов RDD с длинными графами происхождения. А создание контрольных точек удобнее использовать в целях предотвращения отказов и больших затрат на повторные вычисления благодаря сохранению промежуточных результатов [[Литература#^20dbe7]]<c. 140>.

Вообще говоря, мы рекомендуем применять сохранение в случае медленного выполнения заданий, а создание контрольных точек -- когда они не выполняются вообще из-за сбоев [[Литература#^20dbe7]]<c. 143>.

Независимо от вызвов функций `.persist()` и `.checkpoint()`, Spark _все равно во время перетасовки записывает на диск определенные данные_. Файлы этих данных называются _перетасовочными_ и обычно содержат все записи из каждой входной секции, отсортированные подпрограммой отображения. Обычно _перетасовочные файлы_ остаются в локальных каталогах рабочих узлов на все время жизни приложения. Следовательно, при повторном использовании драйверной программой набора RDD, уже подвергнутого перетасовке, Spark получает возможность избежать повторного вычисления этого набора вплоть до состояния на момент перетасовки за счет применения перетасовочных файлов [[Литература#^20dbe7]]<c. 145>.

В отличие от других типов кэширования мы не можем выяснить наличие перетасовычных файлов для конкретного набора RDD, например, не существует эквивалента команды `isCheckPointed`, которая возвращает `true`, если для данного набора RDD была создана контрольная точка. В целом, однако, перетасовочные файлы не очищаются явным образом до момента выхода RDD из области видимости.

Производительность при повторном использовании перетасовочных файлов аналогична производительности кэшируемого только на уровне диска набора RDD [[Литература#^20dbe7]]<c. 146>.

### Работа с данными типа "ключ - значение"

Несмотря на удобство, операции с данными типа "ключ - значение" способны породить множество проблем с производительностью. 

В частности, операции над данными типа "ключ - значение" могут вызывать такие проблемы как:
- ошибки нехватки памяти на драйвере,
- ошибки нехватки памяти на рабочих узлах,
- сбои при перетасовке,
- особенно медленное вычисление отдельных секций.

Первая проблема обычно вызывается действиями. Остальные 3 проблемы с производительностью чаще всего возникают из-за перетасовок при выполнении широких преобразований в классах `PairRDDFunctions` и `OrderedRDDFunctions`.

Две основные методики решения связанных с перетасовкой проблем производительности:
- "Перетасовывать реже". Один из способов минимизировать количество перетасовок в требующем нескольких преобразований вычислении -- сохранять секционирование при узких преобразованиях (чтобы избежать перетасовки данных). 
- "Перетасовывать лучше". Иногда выполнить вычисления без перетасовки невозможно. Однако не все широкие преобразования и перетасовки требуют одинкаовых вычислительных затрат и одинаково подвержены ошибкам. Перетасовка данных с равномерным распределением записей по ключам, причем содержащих большое количество различных ключей, предотвращает ошибки нехватки памяти на рабочих узлах и "отставание задач".