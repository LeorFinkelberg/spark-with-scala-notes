### Выбрать столбцы кадра данных по регулярному выражению

```scala
val columnRegex = """%.*Origin$""".r

df.select(
  df.columns.filter(
    columnRegex.findFirstIn(_).isDefined
).map(col): _*)
```

### Динамический выбор функции агреграции
```scala
import org.apache.spark.sql.functions._

def getAggregate(
  input: DataFrame,
  aggName: String,
): DataFrame = {
  input.select(
    input.columns.map(
      c => call_udf(aggName, col(c))
    ): _*
  )
}
```

### Преобразование строковой даты в Unix-Timestamp (количество секунд с начала эпохи) и обратно

```scala
import org.joda.time.{LocalDateTime, DateTime}

// в секундах!!!
val localDate = new LocalDateTime("2024-08-28").toDateTime().getMillis / 1000
println(localDate)  // 1724792400

val dateTime: DateTime = new DateTime(localDate * 1000L)  // 2024-08-28T00:00:00.000+03:00
```

### Преобразование Scala ассоциативного массива в Java ассоциативный массив

```scala
import java.util
import java.lang
import scala.jdk.CollectionConverters._  // ОЧЕНЬ ВАЖНО!

def convertScalaMapToJavaMap(mapping: Map[Byte, Float]) = {
  mapAsJavaMap(mapping).asInstanceOf[util.Map[lang.Byte, lang.Float]]
}

def logCenterClusters(input: DataFrame, mlflow: MLflowODKL): Unit = {
  for ((clusterIdx, row) <- List.range(0, input.count()).zip(input.rdd.collect())) {
    mlflow.logParams(
      mapAsJavaMap(Map(
        input.columns.map(colName => 
        s"${clusterIdx}_$colName").zip(row.toSeq.toList.map(_.toString)): _*
      ))
    )
  }
}
```

### Сложная схема кадра данных

Пусть есть кадр данных с приведенной ниже схемой
```bash
...
  |-- feedIdMarker: string (nullable = true)
  |-- feedSourceId: long (nullable = true)
  |-- hobbyFeedStatistics: struct (nullable = true)
  |     |-- category: string (nullable = true)
  |     |-- category2: string (nullable = true)
  |     |-- section: string (nullable = true)
  |-- hotNews: boolean (nullable = true)
  |-- reshareOwners: map (nullable = true)
  |     |-- key: string
  |     |-- value: array (valueContainsNull = true)
  |           |-- element: long (nullable = true)
...
```

Такую схему можно описать следующим образом
```scala
val sentSchema: StructType = new StructType()
  .add("feedIdMarker", StringType, nullable = true)
  .add("feedSourceId", LongType, nullable = true)
  .add(
    "hobbyFeedStatistics",
    new StructType()
      .add("category", StringType, nullable = true)
      .add("category2", StringType, nullable = true)
      .add("section", StringType, nullable = true)
  )
  .add("reshareOwners", MapType(StringType, ArrayType(LongType)), nullable = true)
```

Если при обращении в Row-объекте к элементу как `.getAs[Array[T]]` возникает ошибка, то можно попробовать заменить `Array` на `Seq`, то есть `.getAs[Seq[T]]`

Если есть элемент, который представлен как структура,
```scala
root 
  |-- baseProbabilities: struct (nullable = true)
  |     |-- 1: double (nullable = true)
  |     |-- 2: double (nullable = true)
```
то к его элементам можно обратиться так
```scala
for (row <- (...).collect()) {
  println(row.getAs[Row]("baseProbabilities").getAs[Double]("1"))
}
```
### Содержит ли коллекция сразу несколько элементов или нет

Для того чтобы проверить содержит коллекция один элемент, можно использовать `.contains()`
```scala
List("Java", "Scala", "Python").contains("Python")  // true
```

Если же нужно проверить несколько элементов сразу, то можно сделать так
```scala
List("Python", "Java").forall(
  List("Java", "Scala", "Python").contains(_)
) // true
```
### Преобразование элемента-стуктуры в массив

Чтобы преобразовать элемент кадра данных, который представлен структурой, например, элемент `baseProbabilities` в схеме ниже
```bash
root
  |-- afterSwitch: boolean (nullable = true)
  |-- baseProbabilities: struct (nullable = true)
  |     |-- layout1: double (nullable = true)
  |     |-- layout2: double (nullable = true)
  |     |-- layout3: double (nullable = true)
  |-- chunk: long (nullable = true)
  ...
```
то можно сделать так
```bash
data.select(col("afterSwitch"), array("baseProbabilities.*").as("base")).show()
```
### Переименование полей структуры

Иногда поля структуры (например, когда поля структуры представлены числами) бывает нужно переименовать
```scala
def fixSchema(input: DataFrame, colName: String): DataFrame = {
  val schema = input.schema.fields.find(_.name == colName).get.dataType.asInstanceOf[Structtype]
  val newSchema = StructType.apply(schema.fields.map(field => StructField.apply(s"layout${field.name}", field.dataType)))

  input.withColumn(colName, col(colName).cast(newSchema))
}
```
### Пользовательские функции с массивами

Если применение UDF к атрибуту типа `Array[Long]`
```scala
def getSomething = udf {(arr: Array[Long]) => arr.size}
```
вызывает ошибку вида
```bash
org.apache.spark.SparkException: Job aborted ...
- array element class: "scala.Long"
- root class: "scala.Long"
...
```
то можно попробовать использовать Java-Long
```scala
import java.lang

def getSomething = udf { (arr: Array[lang.Long]) => arr.size }
```
### Вычисление моды по атрибуту типа массива

```scala
def getModeUdf = udf {
  (arr: Array[Long]) =>
    arr.filter(_ != 0).groupBy(identity)
      .mapValues(_.length).toList.maxBy(_._2)._1
}
```
### Убрать из атрибута-массива `null`-элементы

Чтобы убрать из атрибута, каждое значение которого представляет собой массив  (например, длинных целых) `null`-элементы, можно воспользоваться функцией `filter`
```scala
data.select(
  filter(array("layoutIdByPosition.*")), x => x.isNotNull).as("res")
)
```

А начиная с версии Spark 3.4, можно использовать функцию `array_compact()`, которая делает тоже самое -- из массива удаляет `null`-элементы.
### Перевести структуру в отображение

Бывает удобно элемент кадра данных, представленный в виде структуры перевести в отображение
```scala
import org.apache.spark.sql.types.{StringType, FloatType, DataTypes}

def convertStructToMap(input: DataFrame): DataFrame = {
  val mapColType = DataTypes.createMapType(StringType, FloatType)

  input.schema.fields
    .filter(_.dataType.typeName.equals("struct"))
    .foldLeft(input) {
      case (tempDf, field) =>
        val colValueAsJson = to_json(col(field.name))
        val colValueAsMap = from_json(colValueAsJson, mapColType)
        tempDf.withColumn(field.name, colValueAsMap)
    }
}
```
### Превратить два столбца в отображение

Собрать два столбца в отображение можно так
```scala
import org.apache.spark.sql.functions.{map_from_etries, col, collect_list, struct}

df.groupBy(...)
  .agg(
    map_from_entries(
      collect_list(struct(col("Col1"), col("Col2"))))
    ).as("name")
```