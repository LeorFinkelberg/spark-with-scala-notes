==NB! Внутри UDF нельзя использоваться Spark-функции типа `lit()`, `col()`, `concat_ws()` etc. Внутри UDF-функции мы работаем как с обычным Scala-кодом. В UDF-фунцкию передается (технически) объект столбца, но внутри функции он рассматривается как значение примитивного типа.
### Сравнение на равенство булевых значений атрибута кадра данных

Сравнить значение булевого атрибута на равенство можно различными способами, но предпочтение следует отдавать либо оператору `===`, либо методу `equalTo()`
```scala
df.filter(col("colName").equalTo(true))
// или
df.filter(col("colName") === true)
```
### Пример функции, работающей в контексте метода `.transform`
```scala
// OkGamesItemBaseJob.scala
object OkGamesItemBaseJob {
  override def run(...) {...}

  def getFeatures(itemCatalog: DataFrame): DataFrame = {
    ...
	inputDataset
	  ...
	  .transform(
	    createCategoryByBitMask(
	      "platforms_from_bitmask",
	      ApplicationPlatform.WEB  // перечисление из Java-кода
		)
	  )
	  ...
	
  private def createCategoryByBitMask
  (
    platformsFromBitMaskColName: String,
    applicationPlatform: ApplicationPlatform,
  ): DataFrame => DataFrame = {
    _.withColumn(
	  s"is_${platformsFromBitMaskColName}_${applicationPlatform.name()}",
	  when(col(platformsFromBitMaskColName).isNull, null)
	    .when(
		  array_contains(
		    col(platformsFromBitMaskColName),
		    applicationPlatform.name()
		  ),
		  1
	    ).otherwise(0)
    )
  }
```

Атрибут `platformsFromBitMaskColName` содержит массивы строк, поэтому в данном случае удобно использовать функцию `array_contains()` для проверки на принадлежность элемента `ApplicationPlatform`.
### Пример UDF-функций

Контекст
```scala
import scala.jdk.CollectionConverters._

import org.apache.spark.sql.expressions.UserDefinedFunction


itemCatalog
  ...
  .col("is_daily_challenge_game"),
  when(
    col("tags").isNotNull,
	concat_ws(",", array_sort(col("tags")))
  ).as("tags_concatenated")
  when(
    col("platforms").isNotNull,
    getPlatformNamesByBitMaskUdf(col("platforms"))
  ).as("platforms_from_bitmask")
  ...

private def getPlatformNamesByBitMaskUdf: UserDefinedFunction = udf {
  (
    bitMask: Long,
  ) => ApplicationPlatform
    .getSetFromMask(bitMask) // возвращает Java-множество
    .asScala  // преобразуем в Scala-множество
    .toArray  // преобразуем в массив
    .map(_.name())
}
```

Интересный вариант проверки на `null`, когда нужно вернуть либо `null`, либо, например, целое число
```scala
inputDataset
  ...
  .withColumn(
    "has_upper_case",
    udf(createUrlFeatureUdf(_.count(_.isUpper)))
	  .apply(col("url"))
  )
  ...
  
private def createUrlFeatureUdf(transform: String => Int): String => Integer = {
  url => Option(url)
    .map(transform)
    .map(_.asInstanceOf[Integer]) // NB! <=
    .orNull
}
```
### Выбрать столбцы кадра данных по регулярному выражению

```scala
val columnRegex = """%.*Origin$""".r

df.select(
  df.columns.filter(
    columnRegex.findFirstIn(_).isDefined
).map(col): _*)
```
### Динамический выбор функции агреграции
```scala
import org.apache.spark.sql.functions._

def getAggregate(
  input: DataFrame,
  aggName: String,
): DataFrame = {
  input.select(
    input.columns.map(
      c => call_udf(aggName, col(c))
    ): _*
  )
}
```
### Собрать значения столбца кадра данных в массив

```scala
df
  .select(col("category")).as[String]
  .collect()
```
Или так
```scala
df
  .select(col("category"))
  .collect()
  .map(row => Option(row.getString(0)).getOrElse(""))
```

И в первом, и во втором случаем получим массив строк `Array[String]`
### Преобразование строковой даты в Unix-Timestamp (количество секунд с начала эпохи) и обратно

```scala
import org.joda.time.{LocalDateTime, DateTime}

// в секундах!!!
val localDate = new LocalDateTime("2024-08-28").toDateTime().getMillis / 1000
println(localDate)  // 1724792400

val dateTime: DateTime = new DateTime(localDate * 1000L)  // 2024-08-28T00:00:00.000+03:00
```

### Преобразование Scala ассоциативного массива в Java ассоциативный массив

```scala
import java.util
import java.lang
import scala.jdk.CollectionConverters._  // ОЧЕНЬ ВАЖНО!

def convertScalaMapToJavaMap(mapping: Map[Byte, Float]) = {
  mapAsJavaMap(mapping).asInstanceOf[util.Map[lang.Byte, lang.Float]]
}

def logCenterClusters(input: DataFrame, mlflow: MLflowODKL): Unit = {
  for ((clusterIdx, row) <- List.range(0, input.count()).zip(input.rdd.collect())) {
    mlflow.logParams(
      mapAsJavaMap(Map(
        input.columns.map(colName => 
        s"${clusterIdx}_$colName").zip(row.toSeq.toList.map(_.toString)): _*
      ))
    )
  }
}
```

### Сложная схема кадра данных

Пусть есть кадр данных с приведенной ниже схемой
```bash
...
  |-- feedIdMarker: string (nullable = true)
  |-- feedSourceId: long (nullable = true)
  |-- hobbyFeedStatistics: struct (nullable = true)
  |     |-- category: string (nullable = true)
  |     |-- category2: string (nullable = true)
  |     |-- section: string (nullable = true)
  |-- hotNews: boolean (nullable = true)
  |-- reshareOwners: map (nullable = true)
  |     |-- key: string
  |     |-- value: array (valueContainsNull = true)
  |           |-- element: long (nullable = true)
...
```

Такую схему можно описать следующим образом
```scala
val sentSchema: StructType = new StructType()
  .add("feedIdMarker", StringType, nullable = true)
  .add("feedSourceId", LongType, nullable = true)
  .add(
    "hobbyFeedStatistics",
    new StructType()
      .add("category", StringType, nullable = true)
      .add("category2", StringType, nullable = true)
      .add("section", StringType, nullable = true)
  )
  .add("reshareOwners", MapType(StringType, ArrayType(LongType)), nullable = true)
```

Если при обращении в Row-объекте к элементу как `.getAs[Array[T]]` возникает ошибка, то можно попробовать заменить `Array` на `Seq`, то есть `.getAs[Seq[T]]`

Если есть элемент, который представлен как структура,
```scala
root 
  |-- baseProbabilities: struct (nullable = true)
  |     |-- 1: double (nullable = true)
  |     |-- 2: double (nullable = true)
```
то к его элементам можно обратиться так
```scala
for (row <- (...).collect()) {
  println(row.getAs[Row]("baseProbabilities").getAs[Double]("1"))
}
```
### Содержит ли коллекция сразу несколько элементов или нет

Для того чтобы проверить содержит коллекция один элемент, можно использовать `.contains()`
```scala
List("Java", "Scala", "Python").contains("Python")  // true
```

Если же нужно проверить несколько элементов сразу, то можно сделать так
```scala
List("Python", "Java").forall(
  List("Java", "Scala", "Python").contains(_)
) // true
```
### Преобразование элемента-стуктуры в массив

Чтобы преобразовать элемент кадра данных, который представлен структурой, например, элемент `baseProbabilities` в схеме ниже
```bash
root
  |-- afterSwitch: boolean (nullable = true)
  |-- baseProbabilities: struct (nullable = true)
  |     |-- layout1: double (nullable = true)
  |     |-- layout2: double (nullable = true)
  |     |-- layout3: double (nullable = true)
  |-- chunk: long (nullable = true)
  ...
```
то можно сделать так
```bash
data.select(col("afterSwitch"), array("baseProbabilities.*").as("base")).show()
```
### Переименование полей структуры

Иногда поля структуры (например, когда поля структуры представлены числами) бывает нужно переименовать
```scala
def fixSchema(input: DataFrame, colName: String): DataFrame = {
  val schema = input.schema.fields.find(_.name == colName).get.dataType.asInstanceOf[Structtype]
  val newSchema = StructType.apply(schema.fields.map(field => StructField.apply(s"layout${field.name}", field.dataType)))

  input.withColumn(colName, col(colName).cast(newSchema))
}
```
### Пользовательские функции с массивами

Если применение UDF к атрибуту типа `Array[Long]`
```scala
def getSomething = udf {(arr: Array[Long]) => arr.size}
```
вызывает ошибку вида
```bash
org.apache.spark.SparkException: Job aborted ...
- array element class: "scala.Long"
- root class: "scala.Long"
...
```
то можно попробовать использовать Java-Long
```scala
import java.lang

def getSomething = udf { (arr: Array[lang.Long]) => arr.size }
```
### Вычисление моды по атрибуту типа массива

```scala
def getModeUdf = udf {
  (arr: Array[Long]) =>
    arr.filter(_ != 0).groupBy(identity)
      .mapValues(_.length).toList.maxBy(_._2)._1
}
```
### Убрать из атрибута-массива `null`-элементы

Чтобы убрать из атрибута, каждое значение которого представляет собой массив  (например, длинных целых) `null`-элементы, можно воспользоваться функцией `filter`
```scala
data.select(
  filter(array("layoutIdByPosition.*")), x => x.isNotNull).as("res")
)
```

А начиная с версии Spark 3.4, можно использовать функцию `array_compact()`, которая делает тоже самое -- из массива удаляет `null`-элементы.
### Перевести структуру в отображение

Бывает удобно элемент кадра данных, представленный в виде структуры перевести в отображение
```scala
import org.apache.spark.sql.types.{StringType, FloatType, DataTypes}

def convertStructToMap(input: DataFrame): DataFrame = {
  val mapColType = DataTypes.createMapType(StringType, FloatType)

  input.schema.fields
    .filter(_.dataType.typeName.equals("struct"))
    .foldLeft(input) {
      case (tempDf, field) =>
        val colValueAsJson = to_json(col(field.name))
        val colValueAsMap = from_json(colValueAsJson, mapColType)
        tempDf.withColumn(field.name, colValueAsMap)
    }
}
```
### Перевести отображение в структуру

```scala
/**
* Это вариант, когда нужно преобразовать какой-то конкретный структурный элемент
*/
private def convertMapToStruct
(
  input: DataFrame,
  colNameWithStructType: String,
): DataFrame = {
  input.schema.fields
    .filter(_.name.equals(colNameWithStructType))
    .foldLeft(input) {
      case (tempDf, field) =>
        val jsonColumn = to_json(col(field.name))
        val jsonStringDataset = input
          .select(jsonColumn)
          .filter(jsonColumn.isNotNull)
          .as[String](Encoders.STRING)

        val jsonSchema = input.sparkSession.read // spark.read
          json(jsonStringDataset)
          .schema

          tempDf.withColumn(field.name, from_json(jsonColumn, jsonSchema))
    }
}
```
### Превратить два столбца в отображение

Собрать два столбца в отображение можно так
```scala
import org.apache.spark.sql.functions.{map_from_etries, col, collect_list, struct}

df.groupBy(...)
  .agg(
    map_from_entries(
      collect_list(struct(col("Col1"), col("Col2"))))
    ).as("name")
```
### Разбить столбец-отображение на отдельные столбцы

```scala
df
  .select(
    col("userId"),  // обязательно нужен атрибут для группировки
    explode(col("baseProbabilities")),
  )
  .groupBy("userId")
  .pivot("key") // 'key' - это стандартное имя для ключей
  .agg(first(col("value"))) // 'value' - это стандартное имя для значений
  .show()
```
### Вычисление аргегата для динамически определяемого набора атрибутов

Иногда бывает необходимо посчитать один и тот же агрегат для группы динамически определяемых атрибутов. 

Например, чтобы посчитать сумму для группы, можно сделать так
```scala
// layoutIdNames: Array[String] = ...
...
  .groupBy(
    "userId",
    "platformType",
    "experiment",
    "segment",
  )
  .agg(
    Map(layoutIdNames.map(colName => colName -> "sum"): _*)
  )
```
### Ковертация отображения из `Map[String, Int]` в `Map[Int, Int]`

Для справки вариант `m.asInstanceOf[Map[Int, Int]]` не будет работать с сортировкой. Будет возникать ошибка приведения строки к целому числу
```scala
val m = Map("1" -> 10, "2" -> 20, "3" -> 30)

m.map { case (key, value) => (key.toInt, value)}.toMap
```
### Сложная UDF и сопоставление с шаблоном

Параметры UDF-функций принимают объекты столбцов (несмотря на то, что в функции указываются простые типы)
```scala
def parseLayoutIdByPositionUdf = udf {
  (
    deepChunks: Array[Boolean],
    layoutIdByPosition: Map[Int, Int],
  ) => {
    val values: layoutIdByPosition.toSeq.sortBy(_._1).map {
      (_, value) => value
    }.toArray
    val distinctValues = values.distinct

    deepChunks match {
      case Array(false) =>
      Map(
        "layoutIdByPositionBase" -> values.toList.head,
        "layoutIdByPositionDeep" -> 0,
      )
      case Array(false, true) =>
      Map(
        ...
      )
    }
  }

df.withColumn(
  "newCol",
  parseLayoutIdByPositionUdf(
    col("deepChunks"),
    col("layoutIdByPosition"),
  )
}
```